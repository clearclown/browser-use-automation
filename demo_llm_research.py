#!/usr/bin/env python3
"""
LLMç ”ç©¶èª¿æŸ»ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¯¾è©±å‹ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ç›´æ¥ç ”ç©¶æƒ…å ±ã‚’è¨­å®šã—ã¦ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import asyncio
import json
from pathlib import Path
from datetime import datetime

from automated_research.llm_provider import get_llm, print_provider_info
from automated_research.prisma_search_strategy import PRISMASearchStrategyGenerator
from automated_research.arxiv_search import ArXivSearcher


async def main():
	"""ãƒ‡ãƒ¢ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
	import os

	print('\n' + '=' * 100)
	print('ğŸ¤– LLMç ”ç©¶èª¿æŸ»ãƒ‡ãƒ¢ - è‡ªå‹•å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰')
	print('=' * 100)
	print('\nãƒ†ãƒ¼ãƒ: Large Language Models (LLM) ã®æœ€æ–°ç ”ç©¶å‹•å‘')
	print('ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: arXiv (å­¦è¡“è«–æ–‡ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆ)')
	print('è«–æ–‡æ•°: æœ€å¤§5ä»¶\n')
	print('=' * 100 + '\n')

	# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±è¡¨ç¤º
	print_provider_info()

	# LLMåˆæœŸåŒ–ï¼ˆç’°å¢ƒå¤‰æ•°LLM_PROVIDERã‚’ä½¿ç”¨ã€æœªè¨­å®šãªã‚‰deepseekï¼‰
	provider = os.getenv('LLM_PROVIDER', 'deepseek')
	print(f'\nğŸ“Š LLMã‚’åˆæœŸåŒ–ä¸­... (Provider: {provider})')
	llm = get_llm(temperature=0.4)
	print(f'âœ… {provider.upper()} ã‚’ä½¿ç”¨ã—ã¾ã™\n')

	# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
	data_dir = Path('automated_research/data')
	reports_dir = Path('automated_research/reports')
	data_dir.mkdir(parents=True, exist_ok=True)
	reports_dir.mkdir(parents=True, exist_ok=True)

	session_id = datetime.now().strftime('%Y%m%d_%H%M%S')

	# ã‚¹ãƒ†ãƒƒãƒ—1: ç ”ç©¶æƒ…å ±ã‚’ç›´æ¥è¨­å®šï¼ˆå¯¾è©±ã‚¹ã‚­ãƒƒãƒ—ï¼‰
	print('=' * 100)
	print('ã‚¹ãƒ†ãƒƒãƒ— 1/3: ç ”ç©¶æƒ…å ±ã®è¨­å®š')
	print('=' * 100 + '\n')

	research_info = {
		'research_topic': 'Large Language Models (LLM) ã®æœ€æ–°ç ”ç©¶å‹•å‘',
		'research_question': 'LLMã®æ€§èƒ½å‘ä¸Šã€åŠ¹ç‡åŒ–ã€å¿œç”¨åˆ†é‡ã®æœ€æ–°æŠ€è¡“ã¯ä½•ã‹ï¼Ÿ',
		'keywords': [
			'Large Language Model',
			'LLM',
			'transformer',
			'GPT',
			'BERT',
			'attention mechanism',
			'language model efficiency',
			'prompt engineering',
		],
		'specific_interests': [
			'LLMã®ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„',
			'è¨ˆç®—åŠ¹ç‡ã®å‘ä¸Šæ‰‹æ³•',
			'Few-shot learning / Zero-shot learning',
			'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°',
			'LLMã®å¿œç”¨åˆ†é‡ï¼ˆã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ãªã©ï¼‰',
		],
		'research_background': 'LLMã¯è‡ªç„¶è¨€èªå‡¦ç†ã®ä¸­æ ¸æŠ€è¡“ã¨ãªã£ã¦ãŠã‚Šã€ChatGPTã€GPT-4ã€Claudeã€Geminiãªã©å¤šæ•°ã®ãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ã—ã¦ã„ã‚‹ã€‚'
		'æœ¬èª¿æŸ»ã§ã¯ã€LLMã®æœ€æ–°ç ”ç©¶å‹•å‘ã€ç‰¹ã«æ€§èƒ½å‘ä¸Šã¨åŠ¹ç‡åŒ–ã«é–¢ã™ã‚‹è«–æ–‡ã‚’ä½“ç³»çš„ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã€‚',
		'year_range': {'start': 2022, 'end': 2025},
		'databases': ['arxiv'],
	}

	# ä¿å­˜
	research_info_path = data_dir / f'research_info_{session_id}.json'
	with open(research_info_path, 'w', encoding='utf-8') as f:
		json.dump(research_info, f, indent=2, ensure_ascii=False)

	print(f'âœ… ç ”ç©¶æƒ…å ±ã‚’è¨­å®š: {research_info_path}')
	print(f'\nãƒˆãƒ”ãƒƒã‚¯: {research_info["research_topic"]}')
	print(f'ç ”ç©¶æœŸé–“: {research_info["year_range"]["start"]}-{research_info["year_range"]["end"]}')
	print(f'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(research_info["keywords"])}å€‹\n')

	# ã‚¹ãƒ†ãƒƒãƒ—2: PRISMAæ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ
	print('=' * 100)
	print('ã‚¹ãƒ†ãƒƒãƒ— 2/3: PRISMAæ¤œç´¢æˆ¦ç•¥ã®ç”Ÿæˆ')
	print('=' * 100 + '\n')

	# LLM APIã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚ã€åŸºæœ¬çš„ãªæ¤œç´¢æˆ¦ç•¥ã‚’ç›´æ¥ä½œæˆ
	print('ğŸ“Š æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­...')

	search_strategy = {
		'search_queries': [
			'Large Language Model AND transformer',
			'LLM AND efficiency',
			'GPT AND attention mechanism',
		],
		'inclusion_criteria': [
			'2022å¹´ä»¥é™ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡',
			'LLMã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€åŠ¹ç‡åŒ–ã€å¿œç”¨ã«é–¢ã™ã‚‹è«–æ–‡',
			'æŸ»èª­æ¸ˆã¿ã¾ãŸã¯ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆã®å­¦è¡“è«–æ–‡',
		],
		'exclusion_criteria': ['è§£èª¬è¨˜äº‹', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«è¨˜äº‹', 'LLMä»¥å¤–ã®ãƒˆãƒ”ãƒƒã‚¯'],
		'year_range': research_info['year_range'],
	}

	# ä¿å­˜
	strategy_path = data_dir / f'search_strategy_{session_id}.json'
	with open(strategy_path, 'w', encoding='utf-8') as f:
		json.dump(search_strategy, f, indent=2, ensure_ascii=False)

	print(f'\nâœ… æ¤œç´¢æˆ¦ç•¥ã‚’ç”Ÿæˆ: {strategy_path}')
	print(f'\nç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªæ•°: {len(search_strategy.get("search_queries", []))}å€‹')
	for i, query in enumerate(search_strategy.get('search_queries', [])[:3], 1):
		print(f'  {i}. {query}')
	if len(search_strategy.get('search_queries', [])) > 3:
		print(f'  ... ä»– {len(search_strategy.get("search_queries", [])) - 3}å€‹\n')

	# ã‚¹ãƒ†ãƒƒãƒ—3: arXivæ¤œç´¢
	print('=' * 100)
	print('ã‚¹ãƒ†ãƒƒãƒ— 3/3: arXivè‡ªå‹•æ¤œç´¢')
	print('=' * 100 + '\n')

	searcher = ArXivSearcher()

	# search_strategyã‚’ä½¿ç”¨ã—ã¦arXivæ¤œç´¢
	print(f'\nğŸ“ arXivæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...')
	try:
		all_papers = await searcher.search(search_strategy=search_strategy, max_results=5)
		print(f'  âœ… {len(all_papers)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹')
	except Exception as e:
		print(f'  âš ï¸  ã‚¨ãƒ©ãƒ¼: {e}')
		all_papers = []

	# é‡è¤‡é™¤å»
	unique_papers = searcher.deduplicate_papers(all_papers)
	unique_papers = unique_papers[:5]  # æœ€å¤§5ä»¶

	print(f'\nâœ… åˆè¨ˆ {len(unique_papers)} ä»¶ã®è«–æ–‡ã‚’åé›†')
	print(f'   ï¼ˆé‡è¤‡é™¤å»å‰: {len(all_papers)} ä»¶ï¼‰\n')

	# ä¿å­˜
	papers_path = data_dir / f'collected_papers_{session_id}.json'
	with open(papers_path, 'w', encoding='utf-8') as f:
		json.dump({'papers': unique_papers, 'total_count': len(unique_papers)}, f, indent=2, ensure_ascii=False)

	print(f'ğŸ’¾ è«–æ–‡æƒ…å ±ã‚’ä¿å­˜: {papers_path}\n')

	# è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
	print('=' * 100)
	print('åé›†ã•ã‚ŒãŸè«–æ–‡ä¸€è¦§')
	print('=' * 100 + '\n')

	for i, paper in enumerate(unique_papers, 1):
		print(f'{i}. {paper.get("title", "Unknown")}')
		print(f'   è‘—è€…: {", ".join(paper.get("authors", ["Unknown"])[:3])}')
		print(f'   ç™ºè¡Œæ—¥: {paper.get("published_date", "Unknown")}')
		print(f'   URL: {paper.get("url", "N/A")}\n')

	# å®Œäº†ã‚µãƒãƒªãƒ¼
	print('=' * 100)
	print('ğŸ‰ ãƒ‡ãƒ¢å®Œäº†ï¼')
	print('=' * 100)
	print('\nç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:')
	print(f'  ğŸ“ ç ”ç©¶æƒ…å ±: {research_info_path}')
	print(f'  ğŸ“Š æ¤œç´¢æˆ¦ç•¥: {strategy_path}')
	print(f'  ğŸ“š åé›†è«–æ–‡: {papers_path}')
	print('\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:')
	print('  - å€‹åˆ¥è«–æ–‡ã®è©³ç´°åˆ†æï¼ˆè½åˆé™½ä¸€å¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼‰')
	print('  - çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ')
	print('  - PRISMAãƒ•ãƒ­ãƒ¼å›³ç”Ÿæˆ')
	print('\nã“ã‚Œã‚‰ã¯å®Œå…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ ã§å®Ÿè¡Œã§ãã¾ã™:')
	print('  uv run python -m automated_research.main --provider deepseek --max-papers 20')
	print('=' * 100 + '\n')


if __name__ == '__main__':
	asyncio.run(main())
