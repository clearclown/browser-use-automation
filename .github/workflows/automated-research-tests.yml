name: Automated Research Tests

permissions:
  actions: read
  contents: write
  pull-requests: write
  issues: write
  statuses: write

# Cancel in-progress runs when a new commit is pushed to the same branch/PR
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

on:
  push:
    branches:
      - main
      - stable
    paths:
      - 'automated_research/**'
      - 'tests/ci/test_arxiv_search.py'
      - 'tests/ci/test_jstage_search.py'
      - 'tests/ci/test_government_documents_search.py'
      - 'tests/ci/test_risk_of_bias.py'
      - 'tests/ci/test_multiple_reviewers.py'
      - 'tests/ci/test_prisma_*.py'
      - 'tests/ci/test_screening_criteria.py'
      - 'tests/integration/test_full_research_workflow.py'
      - '.github/workflows/automated-research-tests.yml'
  pull_request:
    paths:
      - 'automated_research/**'
      - 'tests/ci/test_arxiv_search.py'
      - 'tests/ci/test_jstage_search.py'
      - 'tests/ci/test_government_documents_search.py'
      - 'tests/ci/test_risk_of_bias.py'
      - 'tests/ci/test_multiple_reviewers.py'
      - 'tests/ci/test_prisma_*.py'
      - 'tests/ci/test_screening_criteria.py'
      - 'tests/integration/test_full_research_workflow.py'
      - '.github/workflows/automated-research-tests.yml'
  workflow_dispatch:

jobs:
  # Job 1: Unit tests for each database searcher
  unit-tests:
    name: Unit Tests - ${{ matrix.test-group }}
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      fail-fast: false
      matrix:
        test-group:
          - arxiv
          - jstage
          - government-docs
          - risk-of-bias
          - multiple-reviewers
          - prisma-search

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup UV
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          activate-environment: true

      - name: Install dependencies
        run: uv sync --dev --all-extras

      - name: Run ${{ matrix.test-group }} tests
        run: |
          case "${{ matrix.test-group }}" in
            arxiv)
              uv run pytest -vxs tests/ci/test_arxiv_search.py --tb=short
              ;;
            jstage)
              uv run pytest -vxs tests/ci/test_jstage_search.py --tb=short
              ;;
            government-docs)
              uv run pytest -vxs tests/ci/test_government_documents_search.py --tb=short
              ;;
            risk-of-bias)
              uv run pytest -vxs tests/ci/test_risk_of_bias.py --tb=short
              ;;
            multiple-reviewers)
              uv run pytest -vxs tests/ci/test_multiple_reviewers.py --tb=short
              ;;
            prisma-search)
              uv run pytest -vxs tests/ci/test_prisma_search_strategy.py --tb=short
              ;;
          esac

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-group }}
          path: |
            pytest-report-*.xml
            htmlcov/
          retention-days: 7

  # Job 2: Integration tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: unit-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup UV
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          activate-environment: true

      - name: Install dependencies
        run: uv sync --dev --all-extras

      - name: Run integration tests
        run: |
          uv run pytest -vxs tests/integration/test_full_research_workflow.py --tb=short

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-integration
          path: |
            pytest-report-*.xml
            htmlcov/
          retention-days: 7

  # Job 3: Container build and test
  container-tests:
    name: Podman Container Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: integration-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Install Podman
        run: |
          sudo apt-get update
          sudo apt-get install -y podman
          podman --version

      - name: Build container
        run: |
          podman build -t browser-use-research:test -f Containerfile .

      - name: Test automated research in container
        run: |
          podman run --rm \
            --entrypoint bash \
            -e UV_CACHE_DIR=/tmp/uv-cache \
            browser-use-research:test \
            -c "mkdir -p /tmp/uv-cache && uv run pytest -vxs tests/ci/test_arxiv_search.py tests/ci/test_jstage_search.py tests/ci/test_government_documents_search.py tests/ci/test_risk_of_bias.py tests/ci/test_multiple_reviewers.py --tb=short"

      - name: Test integration in container
        run: |
          podman run --rm \
            --entrypoint bash \
            -e UV_CACHE_DIR=/tmp/uv-cache \
            browser-use-research:test \
            -c "mkdir -p /tmp/uv-cache && uv run pytest -vxs tests/integration/test_full_research_workflow.py --tb=short"

      - name: Check container image size
        run: |
          podman images browser-use-research:test
          echo "Container built and tested successfully!"

  # Job 4: Summary report
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, container-tests]
    if: always()

    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results

      - name: Generate summary
        run: |
          echo "# 🧪 Automated Research Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            echo "✅ **Unit Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Unit Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.integration-tests.result }}" == "success" ]; then
            echo "✅ **Integration Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Integration Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.container-tests.result }}" == "success" ]; then
            echo "✅ **Container Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Container Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Coverage" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- 🔬 arXiv Search: 9 tests" >> $GITHUB_STEP_SUMMARY
          echo "- 🔬 J-STAGE Search: 10 tests" >> $GITHUB_STEP_SUMMARY
          echo "- 🔬 Government Documents: 14 tests" >> $GITHUB_STEP_SUMMARY
          echo "- 🔬 Risk of Bias: 8 tests" >> $GITHUB_STEP_SUMMARY
          echo "- 🔬 Multiple Reviewers: 9 tests" >> $GITHUB_STEP_SUMMARY
          echo "- 🔬 PRISMA Search Strategy: 9 tests" >> $GITHUB_STEP_SUMMARY
          echo "- 🧩 Integration Tests: 5 tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Total**: ~64 automated tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "🎯 **PRISMA 2020 Compliance**: ✅ All components tested" >> $GITHUB_STEP_SUMMARY
