"""
IEEE Xplore Lightweight Searcher

httpx + BeautifulSoupã«ã‚ˆã‚‹è»½é‡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè£…
ãƒ–ãƒ©ã‚¦ã‚¶ä¸ä½¿ç”¨ã§400-600MBã®ãƒªã‚½ãƒ¼ã‚¹å‰Šæ¸›
"""

import asyncio
import re
from typing import Any
from urllib.parse import urlencode

import httpx
from bs4 import BeautifulSoup


class IEEELightweightSearcher:
	"""IEEE Xploreè»½é‡æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ä¸ä½¿ç”¨ï¼‰"""

	def __init__(self, timeout: int = 30):
		"""
		Args:
			timeout: HTTPæ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
		"""
		self.timeout = timeout
		self.base_url = 'https://ieeexplore.ieee.org'

		# ãƒ–ãƒ©ã‚¦ã‚¶ã«è¦‹ã›ã‹ã‘ã‚‹ãƒ˜ãƒƒãƒ€ãƒ¼
		self.headers = {
			'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
			'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
			'Accept-Language': 'en-US,en;q=0.5',
			'Accept-Encoding': 'gzip, deflate',
			'Connection': 'keep-alive',
			'Upgrade-Insecure-Requests': '1',
		}

	async def search_papers(
		self,
		query: str,
		year_start: int | None = None,
		year_end: int | None = None,
		max_results: int = 10,
	) -> list[dict[str, Any]]:
		"""
		IEEE Xploreã§è«–æ–‡ã‚’æ¤œç´¢

		Args:
			query: æ¤œç´¢ã‚¯ã‚¨ãƒª
			year_start: é–‹å§‹å¹´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
			year_end: çµ‚äº†å¹´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
			max_results: æœ€å¤§å–å¾—ä»¶æ•°

		Returns:
			è«–æ–‡æƒ…å ±ã®ãƒªã‚¹ãƒˆ
		"""
		papers = []

		async with httpx.AsyncClient(headers=self.headers, timeout=self.timeout, follow_redirects=True) as client:
			# æ¤œç´¢URLã‚’æ§‹ç¯‰
			search_params = {'queryText': query, 'highlight': 'true', 'returnFacets': 'ALL', 'returnType': 'SEARCH'}

			if year_start and year_end:
				search_params['ranges'] = f'{year_start}_{year_end}_Year'

			search_url = f'{self.base_url}/search/searchresult.jsp?{urlencode(search_params)}'

			try:
				# æ¤œç´¢ãƒšãƒ¼ã‚¸ã‚’å–å¾—
				response = await client.get(search_url)
				response.raise_for_status()

				# HTMLã‚’ãƒ‘ãƒ¼ã‚¹
				soup = BeautifulSoup(response.text, 'html.parser')

				# è«–æ–‡ã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡º
				paper_entries = soup.find_all('div', class_='List-results-items')

				for entry in paper_entries[:max_results]:
					paper = await self._parse_paper_entry(entry, client)
					if paper:
						papers.append(paper)

			except httpx.HTTPStatusError as e:
				print(f'âŒ IEEE Xplore HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code}')
			except httpx.RequestError as e:
				print(f'âŒ IEEE Xplore ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}')
			except Exception as e:
				print(f'âŒ IEEE Xplore äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}')

		return papers

	async def _parse_paper_entry(self, entry: Any, client: httpx.AsyncClient) -> dict[str, Any] | None:
		"""
		è«–æ–‡ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æƒ…å ±ã‚’æŠ½å‡º

		Args:
			entry: BeautifulSoupè«–æ–‡ã‚¨ãƒ³ãƒˆãƒª
			client: httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

		Returns:
			è«–æ–‡æƒ…å ±è¾æ›¸
		"""
		try:
			# ã‚¿ã‚¤ãƒˆãƒ«
			title_tag = entry.find('h2')
			title = title_tag.get_text(strip=True) if title_tag else 'Unknown Title'

			# è«–æ–‡URL
			link_tag = title_tag.find('a') if title_tag else None
			paper_url = f"{self.base_url}{link_tag['href']}" if link_tag and 'href' in link_tag.attrs else None

			# è‘—è€…
			authors_tags = entry.find_all('span', class_='author')
			authors = [author.get_text(strip=True) for author in authors_tags]

			# ç™ºè¡Œæ—¥
			date_tag = entry.find('div', class_='description')
			published_date = None
			if date_tag:
				date_text = date_tag.get_text(strip=True)
				# "Published in: ... Date of Publication: 01 January 2023"ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
				date_match = re.search(r'Date of Publication:\s*(.+)', date_text)
				if date_match:
					published_date = date_match.group(1).strip()

			# ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ï¼‰
			abstract = None
			if paper_url:
				abstract = await self._fetch_abstract(paper_url, client)

			return {
				'title': title,
				'authors': authors,
				'published_date': published_date or 'N/A',
				'url': paper_url or 'N/A',
				'abstract': abstract or 'ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆå–å¾—å¤±æ•—',
				'source': 'IEEE Xplore (Lightweight)',
			}

		except Exception as e:
			print(f'âš ï¸ è«–æ–‡ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}')
			return None

	async def _fetch_abstract(self, paper_url: str, client: httpx.AsyncClient) -> str | None:
		"""
		è«–æ–‡è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’å–å¾—

		Args:
			paper_url: è«–æ–‡URL
			client: httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

		Returns:
			ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
		"""
		try:
			# è©³ç´°ãƒšãƒ¼ã‚¸ã‚’å–å¾—
			response = await client.get(paper_url)
			response.raise_for_status()

			soup = BeautifulSoup(response.text, 'html.parser')

			# ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
			abstract_tag = soup.find('div', class_='abstract-text') or soup.find('div', class_='section', string=re.compile(r'Abstract'))

			if abstract_tag:
				return abstract_tag.get_text(strip=True)

			# JSONãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºï¼ˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
			meta_tag = soup.find('meta', property='og:description')
			if meta_tag and 'content' in meta_tag.attrs:
				return meta_tag['content']

			return None

		except Exception as e:
			print(f'âš ï¸ ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}')
			return None


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
async def _test():
	"""å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ"""
	searcher = IEEELightweightSearcher()

	print('ğŸ” IEEE Xploreè»½é‡æ¤œç´¢ãƒ†ã‚¹ãƒˆ')
	print('æ¤œç´¢ã‚¯ã‚¨ãƒª: "deep learning"')
	print('')

	papers = await searcher.search_papers(query='deep learning', year_start=2022, year_end=2025, max_results=3)

	print(f'âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—\n')

	for idx, paper in enumerate(papers, 1):
		print(f'{idx}. {paper["title"]}')
		print(f'   è‘—è€…: {", ".join(paper["authors"][:3])}')
		print(f'   ç™ºè¡Œæ—¥: {paper["published_date"]}')
		print(f'   URL: {paper["url"]}')
		print('')


if __name__ == '__main__':
	asyncio.run(_test())
