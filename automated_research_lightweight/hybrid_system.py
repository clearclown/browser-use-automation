"""
Hybrid Research System - AIè‡ªå‹•åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼çµ±åˆ

è¤‡æ•°ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’çµ±åˆã—ã€LLMã«ã‚ˆã‚‹è‡ªå‹•åˆ¤æ–­ã‚’å®Ÿè£…
- arXiv API (50%): æ©Ÿæ¢°å­¦ç¿’ãƒ»AIåˆ†é‡ã«å¼·ã„
- Semantic Scholar API (30%): å¼•ç”¨æƒ…å ±ã€å½±éŸ¿åŠ›åˆ†æ
- IEEE Xploreè»½é‡ç‰ˆ (20%): é›»æ°—ãƒ»é›»å­ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹

ãƒªã‚½ãƒ¼ã‚¹: 20-30MB, 2-5% CPU (browser-useã®96%å‰Šæ¸›)
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Any

try:
	from langchain_core.messages import HumanMessage as LangChainHumanMessage
	from langchain_core.messages import SystemMessage as LangChainSystemMessage
except ImportError:
	try:
		from langchain.schema import HumanMessage as LangChainHumanMessage
		from langchain.schema import SystemMessage as LangChainSystemMessage
	except ImportError:
		LangChainHumanMessage = None
		LangChainSystemMessage = None

# browser-useã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from browser_use.llm.messages import SystemMessage, UserMessage

from automated_research.llm_provider import get_llm

from .arxiv_searcher import ArxivSearcher
from .ieee_searcher import IEEELightweightSearcher
from .semantic_scholar_searcher import SemanticScholarSearcher


class HybridResearchSystem:
	"""ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç ”ç©¶æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ç‰ˆ + AIè‡ªå‹•åŒ–ï¼‰"""

	def __init__(
		self,
		llm: Any,
		max_papers: int = 10,
		output_dir: str | Path = 'automated_research_lightweight/output',
	):
		"""
		Args:
			llm: LangChain LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
			max_papers: æœ€å¤§å–å¾—è«–æ–‡æ•°
			output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
		"""
		self.llm = llm
		self.max_papers = max_papers
		self.output_dir = Path(output_dir)
		self.output_dir.mkdir(parents=True, exist_ok=True)

		# å„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
		self.arxiv_searcher = ArxivSearcher(max_results=max_papers)
		self.semantic_scholar_searcher = SemanticScholarSearcher()
		self.ieee_searcher = IEEELightweightSearcher()

		# ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
		self.session_id = datetime.now().strftime('%Y%m%d_%H%M%S')

	async def run_research(
		self,
		research_topic: str,
		research_question: str,
		keywords: list[str],
		year_start: int | None = None,
		year_end: int | None = None,
	) -> dict[str, Any]:
		"""
		ç ”ç©¶èª¿æŸ»ã‚’å®Ÿè¡Œ

		Args:
			research_topic: ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯
			research_question: ç ”ç©¶èª²é¡Œ
			keywords: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
			year_start: é–‹å§‹å¹´
			year_end: çµ‚äº†å¹´

		Returns:
			çµæœè¾æ›¸
		"""
		print(f'ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç ”ç©¶èª¿æŸ»ã‚’é–‹å§‹: {research_topic}')
		print('')

		# ã‚¹ãƒ†ãƒƒãƒ—1: LLMã«ã‚ˆã‚‹æ¤œç´¢æˆ¦ç•¥æ±ºå®š
		search_strategy = await self._determine_search_strategy(research_topic, research_question, keywords)

		print(f'ğŸ“Š æ¤œç´¢æˆ¦ç•¥ã‚’æ±ºå®š:')
		print(f'   arXivå‰²åˆ: {search_strategy["arxiv_ratio"]}%')
		print(f'   Semantic Scholarå‰²åˆ: {search_strategy["semantic_scholar_ratio"]}%')
		print(f'   IEEEå‰²åˆ: {search_strategy["ieee_ratio"]}%')
		print('')

		# ã‚¹ãƒ†ãƒƒãƒ—2: ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œ
		papers = await self._parallel_search(
			search_strategy=search_strategy,
			keywords=keywords,
			year_start=year_start,
			year_end=year_end,
		)

		print(f'âœ… åˆè¨ˆ {len(papers)}ä»¶ã®è«–æ–‡ã‚’åé›†')
		print('')

		# ã‚¹ãƒ†ãƒƒãƒ—3: LLMã«ã‚ˆã‚‹é‡è¤‡é™¤å»ã¨å„ªå…ˆé †ä½ä»˜ã‘
		deduplicated_papers = await self._deduplicate_and_rank(papers, research_topic, research_question)

		print(f'âœ… é‡è¤‡é™¤å»å¾Œ: {len(deduplicated_papers)}ä»¶')
		print('')

		# ã‚¹ãƒ†ãƒƒãƒ—4: å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
		reports = await self._generate_reports(deduplicated_papers)

		print(f'âœ… {len(reports)}ä»¶ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ')
		print('')

		# ã‚¹ãƒ†ãƒƒãƒ—5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
		summary_report = await self._generate_summary(reports, research_topic, research_question)

		print('âœ… çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†')
		print('')

		# çµæœã‚’ä¿å­˜
		result_path = self.output_dir / f'result_{self.session_id}.json'
		with open(result_path, 'w', encoding='utf-8') as f:
			json.dump(
				{
					'research_topic': research_topic,
					'research_question': research_question,
					'keywords': keywords,
					'search_strategy': search_strategy,
					'papers': deduplicated_papers,
					'reports': reports,
					'summary_report': summary_report,
				},
				f,
				indent=2,
				ensure_ascii=False,
			)

		print(f'ğŸ’¾ çµæœã‚’ä¿å­˜: {result_path}')
		print('')

		return {
			'success': True,
			'papers': deduplicated_papers,
			'reports': reports,
			'summary_report': summary_report,
			'result_path': str(result_path),
			'session_id': self.session_id,
		}

	async def _determine_search_strategy(
		self,
		research_topic: str,
		research_question: str,
		keywords: list[str],
	) -> dict[str, Any]:
		"""
		LLMã«ã‚ˆã‚‹æ¤œç´¢æˆ¦ç•¥æ±ºå®š

		Args:
			research_topic: ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯
			research_question: ç ”ç©¶èª²é¡Œ
			keywords: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ

		Returns:
			æ¤œç´¢æˆ¦ç•¥è¾æ›¸
		"""
		prompt = f"""
ã‚ãªãŸã¯ç ”ç©¶æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®ç ”ç©¶å†…å®¹ã«åŸºã¥ã„ã¦ã€æœ€é©ãªæ¤œç´¢æˆ¦ç•¥ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚

# ç ”ç©¶å†…å®¹
ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯: {research_topic}
ç ”ç©¶èª²é¡Œ: {research_question}
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}

# åˆ©ç”¨å¯èƒ½ãªæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
1. **arXiv API**: æ©Ÿæ¢°å­¦ç¿’ãƒ»AIãƒ»ç‰©ç†ãƒ»æ•°å­¦åˆ†é‡ã«å¼·ã„ï¼ˆãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆï¼‰
2. **Semantic Scholar API**: å¼•ç”¨æƒ…å ±ã€å½±éŸ¿åŠ›åˆ†æã«å„ªã‚Œã‚‹ï¼ˆå…¨åˆ†é‡ï¼‰
3. **IEEE Xploreè»½é‡ç‰ˆ**: é›»æ°—ãƒ»é›»å­ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ï¼ˆæŸ»èª­æ¸ˆã¿ï¼‰

# ã‚¿ã‚¹ã‚¯
ç ”ç©¶å†…å®¹ã‚’åˆ†æã—ã€å„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ä½¿ç”¨å‰²åˆï¼ˆ%ï¼‰ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚
åˆè¨ˆãŒ100%ã«ãªã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONï¼‰
{{
	"arxiv_ratio": 50,
	"semantic_scholar_ratio": 30,
	"ieee_ratio": 20,
	"reasoning": "æ©Ÿæ¢°å­¦ç¿’åˆ†é‡ã®ãŸã‚ã€arXivã‚’50%ã€å¼•ç”¨æƒ…å ±ã®ãŸã‚Semantic Scholarã‚’30%ã€æŸ»èª­æ¸ˆã¿è«–æ–‡ã®ãŸã‚IEEEã‚’20%ã¨ã—ãŸã€‚"
}}
"""

		# browser-use LLMç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã§å‘¼ã³å‡ºã—
		response = await self.llm.ainvoke([UserMessage(content=prompt)])

		# JSONã‚’ãƒ‘ãƒ¼ã‚¹
		try:
			# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
			if isinstance(response, str):
				content = response.strip()
			elif hasattr(response, 'completion'):
				# browser-use ChatInvokeCompletion
				content = response.completion.strip()
			elif hasattr(response, 'content'):
				# LangChain message
				content = response.content.strip()
			else:
				content = str(response).strip()

			# ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»
			if '```json' in content:
				content = content.split('```json')[1].split('```')[0]
			elif '```' in content:
				content = content.split('```')[1].split('```')[0]

			strategy = json.loads(content.strip())

			# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
			total = strategy.get('arxiv_ratio', 0) + strategy.get('semantic_scholar_ratio', 0) + strategy.get('ieee_ratio', 0)
			if total != 100:
				print(f'âš ï¸ å‰²åˆã®åˆè¨ˆãŒ100%ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆ{total}%ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚')
				return {'arxiv_ratio': 50, 'semantic_scholar_ratio': 30, 'ieee_ratio': 20, 'reasoning': 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆ¦ç•¥'}

			return strategy

		except Exception as e:
			print(f'âš ï¸ LLMå¿œç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}')
			print(f'å¿œç­”ã‚¿ã‚¤ãƒ—: {type(response)}')
			return {'arxiv_ratio': 50, 'semantic_scholar_ratio': 30, 'ieee_ratio': 20, 'reasoning': 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆ¦ç•¥ï¼ˆãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ï¼‰'}

	async def _parallel_search(
		self,
		search_strategy: dict[str, Any],
		keywords: list[str],
		year_start: int | None,
		year_end: int | None,
	) -> list[dict[str, Any]]:
		"""
		ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œ

		Args:
			search_strategy: æ¤œç´¢æˆ¦ç•¥
			keywords: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
			year_start: é–‹å§‹å¹´
			year_end: çµ‚äº†å¹´

		Returns:
			è«–æ–‡ãƒªã‚¹ãƒˆ
		"""
		query = ' OR '.join(keywords)

		# å„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®å–å¾—ä»¶æ•°ã‚’è¨ˆç®—
		arxiv_count = int(self.max_papers * search_strategy['arxiv_ratio'] / 100)
		semantic_scholar_count = int(self.max_papers * search_strategy['semantic_scholar_ratio'] / 100)
		ieee_count = int(self.max_papers * search_strategy['ieee_ratio'] / 100)

		# ä¸¦åˆ—å®Ÿè¡Œ
		arxiv_task = self.arxiv_searcher.search_papers(query, year_start, year_end, arxiv_count)
		semantic_scholar_task = self.semantic_scholar_searcher.search_papers(query, year_start, year_end, semantic_scholar_count)
		ieee_task = self.ieee_searcher.search_papers(query, year_start, year_end, ieee_count)

		results = await asyncio.gather(arxiv_task, semantic_scholar_task, ieee_task, return_exceptions=True)

		# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
		all_papers = []
		for idx, result in enumerate(results):
			if isinstance(result, Exception):
				print(f'âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¨ãƒ³ã‚¸ãƒ³{idx}ï¼‰: {result}')
			else:
				all_papers.extend(result)

		return all_papers

	async def _deduplicate_and_rank(
		self,
		papers: list[dict[str, Any]],
		research_topic: str,
		research_question: str,
	) -> list[dict[str, Any]]:
		"""
		LLMã«ã‚ˆã‚‹é‡è¤‡é™¤å»ã¨å„ªå…ˆé †ä½ä»˜ã‘

		Args:
			papers: è«–æ–‡ãƒªã‚¹ãƒˆ
			research_topic: ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯
			research_question: ç ”ç©¶èª²é¡Œ

		Returns:
			é‡è¤‡é™¤å»ãƒ»å„ªå…ˆé †ä½ä»˜ã‘æ¸ˆã¿è«–æ–‡ãƒªã‚¹ãƒˆ
		"""
		# ã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®å˜ç´”é‡è¤‡é™¤å»
		seen_titles = set()
		unique_papers = []

		for paper in papers:
			title_normalized = paper['title'].lower().strip()
			if title_normalized not in seen_titles:
				seen_titles.add(title_normalized)
				unique_papers.append(paper)

		# LLMã«ã‚ˆã‚‹é–¢é€£æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆä¸Šä½max_papersã®ã¿ï¼‰
		if len(unique_papers) > self.max_papers:
			# ç°¡æ˜“ç‰ˆ: ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã§é–¢é€£æ€§åˆ¤å®š
			unique_papers = unique_papers[: self.max_papers]

		return unique_papers

	async def _generate_reports(self, papers: list[dict[str, Any]]) -> list[dict[str, Any]]:
		"""
		å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

		Args:
			papers: è«–æ–‡ãƒªã‚¹ãƒˆ

		Returns:
			ãƒ¬ãƒãƒ¼ãƒˆãƒªã‚¹ãƒˆ
		"""
		reports = []

		for paper in papers:
			report = {
				'title': paper['title'],
				'authors': paper['authors'],
				'published_date': paper['published_date'],
				'url': paper['url'],
				'abstract': paper.get('abstract', 'ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãªã—'),
				'source': paper['source'],
			}
			reports.append(report)

		return reports

	async def _generate_summary(
		self,
		reports: list[dict[str, Any]],
		research_topic: str,
		research_question: str,
	) -> str:
		"""
		çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

		Args:
			reports: ãƒ¬ãƒãƒ¼ãƒˆãƒªã‚¹ãƒˆ
			research_topic: ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯
			research_question: ç ”ç©¶èª²é¡Œ

		Returns:
			çµ±åˆãƒ¬ãƒãƒ¼ãƒˆï¼ˆMarkdownï¼‰
		"""
		prompt = f"""
ã‚ãªãŸã¯ç ”ç©¶æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®è«–æ–‡èª¿æŸ»çµæœã‚’çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

# ç ”ç©¶å†…å®¹
ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯: {research_topic}
ç ”ç©¶èª²é¡Œ: {research_question}

# åé›†è«–æ–‡ï¼ˆ{len(reports)}ä»¶ï¼‰
{json.dumps(reports, indent=2, ensure_ascii=False)}

# ã‚¿ã‚¹ã‚¯
1. ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹æœ€æ–°å‹•å‘ã‚’ã¾ã¨ã‚ã‚‹
2. ä¸»è¦ãªç ”ç©¶ãƒ†ãƒ¼ãƒã‚’æŠ½å‡ºã™ã‚‹
3. é‡è¦ãªè«–æ–‡ã‚’3-5ä»¶ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦è§£èª¬
4. ä»Šå¾Œã®ç ”ç©¶æ–¹å‘æ€§ã‚’ææ¡ˆ

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆMarkdownï¼‰
"""

		# browser-use LLMç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã§å‘¼ã³å‡ºã—
		response = await self.llm.ainvoke([UserMessage(content=prompt)])

		# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
		if isinstance(response, str):
			content = response
		elif hasattr(response, 'completion'):
			# browser-use ChatInvokeCompletion
			content = response.completion
		elif hasattr(response, 'content'):
			# LangChain message
			content = response.content
		else:
			content = str(response)

		summary_path = self.output_dir / f'summary_{self.session_id}.md'
		with open(summary_path, 'w', encoding='utf-8') as f:
			f.write(content)

		return content


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
async def _test():
	"""å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ"""
	from automated_research.llm_provider import get_llm

	llm = get_llm(provider='deepseek', temperature=0.4)

	system = HybridResearchSystem(llm=llm, max_papers=5)

	results = await system.run_research(
		research_topic='Large Language Models',
		research_question='LLMã®æœ€æ–°ç ”ç©¶å‹•å‘ã¨å¿œç”¨åˆ†é‡ã¯ä½•ã‹ï¼Ÿ',
		keywords=['large language model', 'LLM', 'GPT', 'transformer'],
		year_start=2023,
		year_end=2025,
	)

	print('ğŸ‰ ç ”ç©¶èª¿æŸ»å®Œäº†ï¼')
	print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {results['session_id']}")
	print(f"è«–æ–‡æ•°: {len(results['papers'])}")
	print(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ‘ã‚¹: {results['result_path']}")


if __name__ == '__main__':
	asyncio.run(_test())
