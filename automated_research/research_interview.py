"""
Interactive research interview system
å¯¾è©±å‹ç ”ç©¶ãƒ’ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import json
import logging
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

from browser_use.llm.openai.chat import ChatOpenAI

load_dotenv()

logger = logging.getLogger(__name__)


class ResearchInterviewer:
	"""ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶å†…å®¹ã‚’ãƒ’ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ """

	def __init__(self, llm: ChatOpenAI | None = None):
		self.llm = llm or ChatOpenAI(model='gpt-4o', temperature=0.7)
		self.conversation_history: list[dict[str, str]] = []

	async def conduct_interview(self) -> dict[str, Any]:
		"""
		å¯¾è©±å½¢å¼ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶å†…å®¹ã‚’ãƒ’ã‚¢ãƒªãƒ³ã‚°

		Returns:
			ç ”ç©¶æƒ…å ±ã‚’å«ã‚€è¾æ›¸
		"""
		print('\n' + '=' * 80)
		print('ğŸ”¬ ç ”ç©¶ãƒ†ãƒ¼ãƒè‡ªå‹•åˆ†æã‚·ã‚¹ãƒ†ãƒ ')
		print('=' * 80)
		print('\nã©ã‚“ãªå†…å®¹ã«ã¤ã„ã¦æ¤œç´¢ã—ã¾ã™ã‹ï¼Ÿã‚ãªãŸã®ç ”ç©¶ã¯ãªã‚“ã§ã™ã‹ï¼Ÿ')
		print('ã‚ãªãŸã®ç ”ç©¶ã«ã¤ã„ã¦è‡ªç”±ã«èªã£ã¦ãã ã•ã„ã€‚')
		print('ï¼ˆè©³ã—ã‘ã‚Œã°è©³ã—ã„ã»ã©ã€ã‚ˆã‚Šé©åˆ‡ãªæ–‡çŒ®æ¤œç´¢ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼‰')
		print('\n' + '-' * 80 + '\n')

		# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆæœŸå…¥åŠ›ã‚’å–å¾—
		user_initial_input = input('ã‚ãªãŸ: ').strip()

		if not user_initial_input:
			print('âš ï¸  å…¥åŠ›ãŒç©ºã§ã™ã€‚ç ”ç©¶å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
			return await self.conduct_interview()

		# ç ”ç©¶æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
		research_info = {
			'initial_description': user_initial_input,
			'research_theme': '',
			'research_purpose': '',
			'research_field': '',
			'specific_technologies': [],
			'problem_statement': '',
			'known_papers': [],
			'additional_context': '',
		}

		print('\nğŸ“ ã‚ãªãŸã®ç ”ç©¶å†…å®¹ã‚’åˆ†æã—ã¦ã„ã¾ã™...\n')

		# LLMã«ç ”ç©¶å†…å®¹ã‚’åˆ†æã•ã›ã€è¿½åŠ è³ªå•ã‚’ç”Ÿæˆ
		analysis_prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»¥ä¸‹ã®ã‚ˆã†ã«ç ”ç©¶å†…å®¹ã‚’èª¬æ˜ã—ã¾ã—ãŸï¼š

ã€Œ{user_initial_input}ã€

ã“ã®èª¬æ˜ã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
1. ç ”ç©¶ãƒ†ãƒ¼ãƒã®ä»®èª¬
2. ç ”ç©¶åˆ†é‡ã®æ¨å®š
3. æ˜ç¢ºåŒ–ãŒå¿…è¦ãªç‚¹ï¼ˆ2-3å€‹ã®å…·ä½“çš„ãªè³ªå•ï¼‰

JSONå½¢å¼ã§ä»¥ä¸‹ã®æ§‹é€ ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
	"inferred_theme": "æ¨å®šã•ã‚Œã‚‹ç ”ç©¶ãƒ†ãƒ¼ãƒ",
	"inferred_field": "æ¨å®šã•ã‚Œã‚‹ç ”ç©¶åˆ†é‡",
	"clarification_questions": [
		"è³ªå•1",
		"è³ªå•2",
		"è³ªå•3"
	]
}}
"""

		# LLMã‚’ä½¿ã£ã¦åˆ†æ
		from browser_use.llm.messages import UserMessage

		messages = [UserMessage(content=analysis_prompt)]

		try:
			response = await self.llm.get_response(messages)
			response_text = response.content

			# JSONã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ï¼‰
			if '```json' in response_text:
				json_text = response_text.split('```json')[1].split('```')[0].strip()
			elif '```' in response_text:
				json_text = response_text.split('```')[1].split('```')[0].strip()
			else:
				json_text = response_text.strip()

			analysis = json.loads(json_text)

			research_info['research_theme'] = analysis.get('inferred_theme', '')
			research_info['research_field'] = analysis.get('inferred_field', '')

			print(f'æ¨å®šã•ã‚Œã‚‹ç ”ç©¶ãƒ†ãƒ¼ãƒ: {analysis.get("inferred_theme", "ä¸æ˜")}')
			print(f'æ¨å®šã•ã‚Œã‚‹ç ”ç©¶åˆ†é‡: {analysis.get("inferred_field", "ä¸æ˜")}\n')

			# è¿½åŠ ã®è³ªå•ã‚’ã™ã‚‹
			print('ã‚ˆã‚Šé©åˆ‡ãªæ¤œç´¢ã®ãŸã‚ã€ã„ãã¤ã‹è³ªå•ã•ã›ã¦ãã ã•ã„ï¼š\n')

			clarification_questions = analysis.get('clarification_questions', [])

			for i, question in enumerate(clarification_questions, 1):
				print(f'è³ªå• {i}: {question}')
				answer = input('ã‚ãªãŸ: ').strip()

				if answer:
					# è³ªå•ã«å¿œã˜ã¦æƒ…å ±ã‚’æ ¼ç´
					if i == 1:
						research_info['research_purpose'] = answer
					elif i == 2:
						research_info['problem_statement'] = answer
					elif i == 3:
						research_info['additional_context'] = answer

				print()

			# ç‰¹å®šã®æŠ€è¡“ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¢ºèª
			print('è³ªå•: ç‰¹ã«æ³¨ç›®ã—ã¦ã„ã‚‹æŠ€è¡“ã€æ‰‹æ³•ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„')
			print('     ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ï¼‰')
			tech_input = input('ã‚ãªãŸ: ').strip()

			if tech_input:
				research_info['specific_technologies'] = [t.strip() for t in tech_input.split(',') if t.strip()]

			print()

			# æ—¢çŸ¥ã®è«–æ–‡ãƒ»ç ”ç©¶è€…
			print('è³ªå•: ã™ã§ã«çŸ¥ã£ã¦ã„ã‚‹é‡è¦ãªè«–æ–‡ã‚„ç ”ç©¶è€…ã¯ã„ã¾ã™ã‹ï¼Ÿ')
			print('     ï¼ˆä»»æ„ï¼šã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã¯ Enter ã‚’æŠ¼ã—ã¦ãã ã•ã„ï¼‰')
			papers_input = input('ã‚ãªãŸ: ').strip()

			if papers_input:
				research_info['known_papers'] = [p.strip() for p in papers_input.split(',') if p.strip()]

		except Exception as e:
			logger.error(f'Error during interview analysis: {e}')
			# ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯åŸºæœ¬æƒ…å ±ã®ã¿ã§ç¶šè¡Œ
			research_info['research_theme'] = user_initial_input
			research_info['research_field'] = 'æœªåˆ†é¡'

		print('\n' + '=' * 80)
		print('âœ… ãƒ’ã‚¢ãƒªãƒ³ã‚°å®Œäº†ï¼')
		print('=' * 80)
		print('\nåé›†ã—ãŸç ”ç©¶æƒ…å ±:')
		print(f'- ç ”ç©¶ãƒ†ãƒ¼ãƒ: {research_info["research_theme"]}')
		print(f'- ç ”ç©¶åˆ†é‡: {research_info["research_field"]}')
		if research_info['specific_technologies']:
			print(f'- æ³¨ç›®æŠ€è¡“: {", ".join(research_info["specific_technologies"])}')
		print()

		return research_info

	def save_research_info(self, research_info: dict[str, Any], output_path: Path) -> None:
		"""ç ”ç©¶æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
		output_path.parent.mkdir(parents=True, exist_ok=True)

		with open(output_path, 'w', encoding='utf-8') as f:
			json.dump(research_info, f, indent=2, ensure_ascii=False)

		logger.info(f'Research info saved to: {output_path}')


async def main():
	"""ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
	interviewer = ResearchInterviewer()
	research_info = await interviewer.conduct_interview()

	# ä¿å­˜
	output_path = Path('automated_research/data/research_info.json')
	interviewer.save_research_info(research_info, output_path)

	print(f'\nğŸ’¾ ç ”ç©¶æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}')


if __name__ == '__main__':
	logging.basicConfig(level=logging.INFO)
	asyncio.run(main())
