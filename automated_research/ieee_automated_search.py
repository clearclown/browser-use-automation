"""
Automated IEEE search with browser-use and PDF extraction
browser-useã‚’ä½¿ã£ãŸIEEEè‡ªå‹•æ¤œç´¢ã¨PDFå–å¾—
"""

import asyncio
import json
import logging
import re
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

from browser_use import Agent
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.llm.openai.chat import ChatOpenAI

load_dotenv()

logger = logging.getLogger(__name__)


class IEEEAutomatedSearcher:
	"""IEEE Xploreã§ã®è‡ªå‹•æ¤œç´¢ã¨PDFå–å¾—"""

	def __init__(self, llm: ChatOpenAI | None = None, headless: bool = False):
		self.llm = llm or ChatOpenAI(model='gpt-4o', temperature=0.0)
		self.headless = headless

	async def search_and_collect(
		self, search_strategy: dict[str, Any], research_info: dict[str, Any], max_papers: int = 20
	) -> list[dict[str, Any]]:
		"""
		æ¤œç´¢æˆ¦ç•¥ã«åŸºã¥ã„ã¦IEEE Xploreã‚’æ¤œç´¢ã—ã€è«–æ–‡æƒ…å ±ã‚’åé›†

		Args:
			search_strategy: PRISMAæ–¹å¼ã®æ¤œç´¢æˆ¦ç•¥
			research_info: ç ”ç©¶æƒ…å ±
			max_papers: åé›†ã™ã‚‹æœ€å¤§è«–æ–‡æ•°

		Returns:
			è«–æ–‡æƒ…å ±ã®ãƒªã‚¹ãƒˆ
		"""
		print('\n' + '=' * 80)
		print('ğŸ” IEEE Xploreè‡ªå‹•æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™')
		print('=' * 80 + '\n')

		all_papers = []
		search_queries = search_strategy.get('search_queries', [])

		if not search_queries:
			logger.warning('No search queries found in strategy')
			return all_papers

		# ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
		profile = BrowserProfile(
			headless=self.headless,
			disable_security=False,
			extra_chromium_args=[
				'--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
			],
		)
		browser_session = BrowserSession(browser_profile=profile)

		try:
			# å„æ¤œç´¢ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã‚’å®Ÿè¡Œ
			for query_idx, query in enumerate(search_queries, 1):
				print(f'\nğŸ“ æ¤œç´¢ {query_idx}/{len(search_queries)}: {query}')

				# æ¤œç´¢ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
				papers_per_query = max(5, max_papers // len(search_queries))
				search_task = self._build_search_task(query, papers_per_query, search_strategy)

				# Agentã‚’ä½¿ã£ã¦æ¤œç´¢
				agent = Agent(
					task=search_task,
					llm=self.llm,
					browser_session=browser_session,
					max_actions_per_step=15,
				)

				try:
					history = await agent.run(max_steps=30)

					# çµæœã‚’æŠ½å‡º
					papers = self._extract_papers_from_history(history, query)

					if papers:
						print(f'  âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹')
						all_papers.extend(papers)
					else:
						print('  âš ï¸  è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')

				except Exception as e:
					logger.error(f'Error during search for query "{query}": {e}')
					continue

				# ä¸Šé™ã«é”ã—ãŸã‚‰çµ‚äº†
				if len(all_papers) >= max_papers:
					print(f'\nğŸ¯ ç›®æ¨™è«–æ–‡æ•° {max_papers} ã«åˆ°é”ã—ã¾ã—ãŸ')
					break

				# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
				await asyncio.sleep(2)

		finally:
			await browser_session.kill()

		# é‡è¤‡ã‚’é™¤å»
		unique_papers = self._deduplicate_papers(all_papers)

		print(f'\nâœ… åˆè¨ˆ {len(unique_papers)} ä»¶ã®è«–æ–‡ã‚’åé›†ã—ã¾ã—ãŸ')
		print(f'   ï¼ˆé‡è¤‡é™¤å»å‰: {len(all_papers)} ä»¶ï¼‰')

		return unique_papers[:max_papers]

	def _build_search_task(self, query: str, max_results: int, search_strategy: dict[str, Any]) -> str:
		"""Agentã«æ¸¡ã™æ¤œç´¢ã‚¿ã‚¹ã‚¯ã‚’æ§‹ç¯‰"""
		year_range = search_strategy.get('year_range', {})
		start_year = year_range.get('start', 2018)
		end_year = year_range.get('end', 2024)

		task = f"""
IEEE Xploreã§ä»¥ä¸‹ã®æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€è«–æ–‡æƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š

æ¤œç´¢ã‚¯ã‚¨ãƒª: "{query}"
å‡ºç‰ˆå¹´ç¯„å›²: {start_year} - {end_year}
å–å¾—ä»¶æ•°: æœ€å¤§{max_results}ä»¶

æ‰‹é †:
1. IEEE Xplore (https://ieeexplore.ieee.org/) ã«ã‚¢ã‚¯ã‚»ã‚¹
2. æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã« "{query}" ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢
3. å‡ºç‰ˆå¹´ãƒ•ã‚£ãƒ«ã‚¿ã§ {start_year}-{end_year} ã‚’è¨­å®š
4. æ¤œç´¢çµæœã‹ã‚‰ä¸Šä½{max_results}ä»¶ã®è«–æ–‡ã«ã¤ã„ã¦ä»¥ä¸‹ã®æƒ…å ±ã‚’åé›†:
   - ã‚¿ã‚¤ãƒˆãƒ«
   - è‘—è€…
   - å‡ºç‰ˆå¹´
   - å‡ºç‰ˆç‰©åï¼ˆJournal/Conferenceåï¼‰
   - DOI
   - Abstract
   - è«–æ–‡URL
   - PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰

5. åé›†ã—ãŸæƒ…å ±ã‚’JSONå½¢å¼ã§ä»¥ä¸‹ã®ã‚ˆã†ã«å‡ºåŠ›:
{{
  "papers": [
    {{
      "title": "è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«",
      "authors": ["è‘—è€…1", "è‘—è€…2"],
      "year": 2023,
      "publication": "Journal/Conferenceå",
      "doi": "10.1109/...",
      "abstract": "è¦ç´„æ–‡",
      "url": "https://ieeexplore.ieee.org/document/...",
      "pdf_url": "PDF URL if available"
    }}
  ]
}}

æ³¨æ„:
- å„è«–æ–‡ã®ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«è¨ªå•ã—ã¦è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- æƒ…å ±ãŒå–å¾—ã§ããªã„é …ç›®ã¯ "N/A" ã¨ã—ã¦ãã ã•ã„
- PDFã¯ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã›ãšã€URLã®ã¿è¨˜éŒ²ã—ã¦ãã ã•ã„
"""
		return task

	def _extract_papers_from_history(self, history: Any, query: str) -> list[dict[str, Any]]:
		"""Agentå®Ÿè¡Œå±¥æ­´ã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’æŠ½å‡º"""
		papers = []

		# historyã‹ã‚‰æœ€çµ‚çµæœã‚’å–å¾—
		if hasattr(history, 'final_result') and history.final_result:
			result_text = str(history.final_result())

			# JSONå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
			try:
				# JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
				json_match = re.search(r'\{[\s\S]*"papers"[\s\S]*\}', result_text)
				if json_match:
					data = json.loads(json_match.group(0))
					papers = data.get('papers', [])

					# å„è«–æ–‡ã«æ¤œç´¢ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’è¿½åŠ 
					for paper in papers:
						paper['search_query'] = query

			except json.JSONDecodeError as e:
				logger.warning(f'Failed to parse JSON from result: {e}')

		# historyã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ä»£æ›¿æ‰‹æ®µ
		if not papers and hasattr(history, 'history'):
			for step in history.history:
				if hasattr(step, 'result') and step.result:
					for result in step.result:
						if hasattr(result, 'extracted_content'):
							content = result.extracted_content
							if content and 'title' in content.lower():
								# Try to extract paper info from content
								try:
									paper_data = self._parse_paper_info(content)
									if paper_data:
										paper_data['search_query'] = query
										papers.append(paper_data)
								except Exception as e:
									logger.debug(f'Failed to parse paper info: {e}')

		return papers

	def _parse_paper_info(self, content: str) -> dict[str, Any] | None:
		"""ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
		# ç°¡æ˜“çš„ãªãƒ‘ãƒ¼ã‚¹å‡¦ç†
		# å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šå …ç‰¢ãªãƒ‘ãƒ¼ã‚¹ãŒå¿…è¦
		try:
			# JSONå½¢å¼ã‚’æ¢ã™
			json_match = re.search(r'\{[^}]+\}', content)
			if json_match:
				return json.loads(json_match.group(0))
		except (json.JSONDecodeError, AttributeError):
			pass

		return None

	def _deduplicate_papers(self, papers: list[dict[str, Any]]) -> list[dict[str, Any]]:
		"""ã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã§è«–æ–‡ã®é‡è¤‡ã‚’é™¤å»"""
		seen_titles = set()
		unique_papers = []

		for paper in papers:
			title = paper.get('title', '').lower().strip()
			if title and title not in seen_titles:
				seen_titles.add(title)
				unique_papers.append(paper)

		return unique_papers

	def save_papers(self, papers: list[dict[str, Any]], output_path: Path) -> None:
		"""è«–æ–‡æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
		output_path.parent.mkdir(parents=True, exist_ok=True)

		with open(output_path, 'w', encoding='utf-8') as f:
			json.dump({'papers': papers, 'total_count': len(papers)}, f, indent=2, ensure_ascii=False)

		logger.info(f'Papers saved to: {output_path}')


async def main():
	"""ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
	# æ¤œç´¢æˆ¦ç•¥ã‚’èª­ã¿è¾¼ã¿
	strategy_path = Path('automated_research/data/search_strategy.json')
	research_info_path = Path('automated_research/data/research_info.json')

	if not strategy_path.exists():
		print('âš ï¸  æ¤œç´¢æˆ¦ç•¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')
		print('å…ˆã« prisma_search_strategy.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚')
		return

	if not research_info_path.exists():
		print('âš ï¸  ç ”ç©¶æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')
		return

	with open(strategy_path, encoding='utf-8') as f:
		search_strategy = json.load(f)

	with open(research_info_path, encoding='utf-8') as f:
		research_info = json.load(f)

	# æ¤œç´¢ã‚’å®Ÿè¡Œ
	searcher = IEEEAutomatedSearcher(headless=False)
	papers = await searcher.search_and_collect(search_strategy, research_info, max_papers=10)

	# ä¿å­˜
	output_path = Path('automated_research/data/collected_papers.json')
	searcher.save_papers(papers, output_path)

	print(f'\nğŸ’¾ è«–æ–‡æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}')


if __name__ == '__main__':
	logging.basicConfig(level=logging.INFO)
	asyncio.run(main())
