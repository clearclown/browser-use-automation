#!/usr/bin/env python3
"""
Automated Research Assistant - Main Entry Point
å®Œå…¨è‡ªå‹•åŒ–ç ”ç©¶æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ï¼š
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å¯¾è©±å‹ãƒ’ã‚¢ãƒªãƒ³ã‚°
2. PRISMAæ–¹å¼ã®æ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ
3. IEEE Xploreã§ã®è‡ªå‹•æ¤œç´¢ã¨è«–æ–‡åé›†
4. å„è«–æ–‡ã®è½åˆé™½ä¸€å¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
5. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ

Usage:
    python -m automated_research.main

    ã¾ãŸã¯

    python automated_research/main.py
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

from automated_research.llm_provider import get_llm, print_provider_info
from automated_research.ochiai_report_generator import OchiaiReportGenerator
from automated_research.prisma_search_strategy import PRISMASearchStrategyGenerator
from automated_research.research_interview import ResearchInterviewer
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.integrations.ieee_search import IEEESearchService

load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')

logger = logging.getLogger(__name__)


class AutomatedResearchAssistant:
	"""å®Œå…¨è‡ªå‹•åŒ–ç ”ç©¶æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ """

	def __init__(
		self, llm: Any | None = None, headless: bool = False, max_papers: int = 20, non_interactive: bool = False, research_topic: str | None = None
	):
		"""
		Initialize the automated research assistant

		Args:
			llm: Language model instance (if None, will use get_llm() to auto-select from env)
			headless: Run browser in headless mode
			max_papers: Maximum number of papers to collect
			non_interactive: Skip interactive interview and use predefined research info
			research_topic: Research topic for non-interactive mode
		"""
		self.llm = llm or get_llm(temperature=0.4)
		self.headless = headless
		self.max_papers = max_papers
		self.non_interactive = non_interactive
		self.research_topic = research_topic

		# ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
		self.base_dir = Path('automated_research')
		self.data_dir = self.base_dir / 'data'
		self.reports_dir = self.base_dir / 'reports'
		self.logs_dir = self.base_dir / 'logs'

		# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
		for dir_path in [self.data_dir, self.reports_dir, self.logs_dir]:
			dir_path.mkdir(parents=True, exist_ok=True)

		# ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ï¼‰
		self.session_id = datetime.now().strftime('%Y%m%d_%H%M%S')

	async def run_full_pipeline(self) -> None:
		"""å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
		print('\n' + '=' * 100)
		print('ğŸ¤– å®Œå…¨è‡ªå‹•åŒ–ç ”ç©¶æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ')
		print('=' * 100)
		print('\nã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ï¼š')
		print('  1. ç ”ç©¶å†…å®¹ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°')
		print('  2. PRISMAæ–¹å¼ã®æ¤œç´¢æˆ¦ç•¥ç«‹æ¡ˆ')
		print('  3. IEEE Xploreã§ã®è‡ªå‹•æ¤œç´¢')
		print('  4. è«–æ–‡ã®è©³ç´°åˆ†æï¼ˆè½åˆé™½ä¸€å¼ï¼‰')
		print('  5. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ')
		print('\n' + '=' * 100 + '\n')

		try:
			# ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ’ã‚¢ãƒªãƒ³ã‚°
			research_info = await self._step1_interview()

			# ã‚¹ãƒ†ãƒƒãƒ—2: PRISMAæ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ
			search_strategy = await self._step2_generate_strategy(research_info)

			# ã‚¹ãƒ†ãƒƒãƒ—3: IEEEè‡ªå‹•æ¤œç´¢
			papers = await self._step3_search_papers(search_strategy)

			if not papers:
				print('\nâš ï¸  è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚')
				return

			# ã‚¹ãƒ†ãƒƒãƒ—4: å„è«–æ–‡ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
			reports = await self._step4_generate_reports(papers, research_info)

			# ã‚¹ãƒ†ãƒƒãƒ—5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
			await self._step5_generate_summary(reports, research_info, search_strategy)

			# å®Œäº†
			self._print_completion_summary()

		except KeyboardInterrupt:
			print('\n\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚')
			logger.info('Process interrupted by user')
		except Exception as e:
			print(f'\n\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')
			logger.error(f'Fatal error in pipeline: {e}', exc_info=True)
			raise

	async def _step1_interview(self) -> dict[str, Any]:
		"""ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ’ã‚¢ãƒªãƒ³ã‚°"""
		print('\n' + 'ğŸ¯ ' * 30)
		print('ã‚¹ãƒ†ãƒƒãƒ— 1/5: ç ”ç©¶å†…å®¹ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°')
		print('ğŸ¯ ' * 30 + '\n')

		if self.non_interactive:
			# éå¯¾è©±å‹ãƒ¢ãƒ¼ãƒ‰ï¼šäº‹å‰å®šç¾©ã•ã‚ŒãŸç ”ç©¶æƒ…å ±ã‚’ä½¿ç”¨
			print('ğŸ“ éå¯¾è©±å‹ãƒ¢ãƒ¼ãƒ‰ï¼šäº‹å‰å®šç¾©ã•ã‚ŒãŸç ”ç©¶æƒ…å ±ã‚’ä½¿ç”¨\n')

			topic = self.research_topic or 'Large Language Models (LLM) ã®æœ€æ–°ç ”ç©¶å‹•å‘'
			research_info = {
				'research_topic': topic,
				'research_question': f'{topic}ã«ãŠã‘ã‚‹æœ€æ–°æŠ€è¡“ã¨å¿œç”¨åˆ†é‡ã¯ä½•ã‹ï¼Ÿ',
				'keywords': [
					'large language model',
					'LLM',
					'transformer',
					'neural network',
					'deep learning',
					'natural language processing',
				],
				'specific_interests': [
					'æœ€æ–°ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„',
					'åŠ¹ç‡åŒ–æ‰‹æ³•',
					'å¿œç”¨åˆ†é‡',
					'æ€§èƒ½å‘ä¸ŠæŠ€è¡“',
				],
				'research_background': f'{topic}ã®ç ”ç©¶å‹•å‘ã‚’ä½“ç³»çš„ã«èª¿æŸ»ã™ã‚‹ã€‚',
				'year_range': {'start': 2022, 'end': 2025},
				'databases': ['ieee'],
			}

			print(f'âœ… ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯: {research_info["research_topic"]}')
			print(f'âœ… ç ”ç©¶æœŸé–“: {research_info["year_range"]["start"]}-{research_info["year_range"]["end"]}\n')
		else:
			# å¯¾è©±å‹ãƒ¢ãƒ¼ãƒ‰
			interviewer = ResearchInterviewer(llm=self.llm)
			research_info = await interviewer.conduct_interview()

		# ä¿å­˜
		output_path = self.data_dir / f'research_info_{self.session_id}.json'
		with open(output_path, 'w', encoding='utf-8') as f:
			json.dump(research_info, f, indent=2, ensure_ascii=False)

		print(f'ğŸ’¾ ç ”ç©¶æƒ…å ±ã‚’ä¿å­˜: {output_path}\n')

		return research_info

	async def _step2_generate_strategy(self, research_info: dict[str, Any]) -> dict[str, Any]:
		"""ã‚¹ãƒ†ãƒƒãƒ—2: PRISMAæ¤œç´¢æˆ¦ç•¥ç”Ÿæˆ"""
		print('\n' + 'ğŸ“Š ' * 30)
		print('ã‚¹ãƒ†ãƒƒãƒ— 2/5: PRISMAæ–¹å¼æ¤œç´¢æˆ¦ç•¥ã®ç”Ÿæˆ')
		print('ğŸ“Š ' * 30 + '\n')

		generator = PRISMASearchStrategyGenerator(llm=self.llm)
		search_strategy = await generator.generate_search_strategy(research_info)

		# ä¿å­˜
		output_path = self.data_dir / f'search_strategy_{self.session_id}.json'
		generator.save_search_strategy(search_strategy, output_path)

		return search_strategy

	async def _step3_search_papers(self, search_strategy: dict[str, Any]) -> list[dict[str, Any]]:
		"""ã‚¹ãƒ†ãƒƒãƒ—3: IEEEè‡ªå‹•æ¤œç´¢"""
		print('\n' + 'ğŸ” ' * 30)
		print('ã‚¹ãƒ†ãƒƒãƒ— 3/5: IEEE Xploreã§ã®è‡ªå‹•æ¤œç´¢')
		print('ğŸ” ' * 30 + '\n')

		search_queries = search_strategy.get('search_queries', [])
		if not search_queries:
			logger.warning('No search queries found')
			return []

		# ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
		profile = BrowserProfile(
			headless=self.headless,
			disable_security=False,
			extra_chromium_args=[
				'--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
			],
		)
		browser_session = BrowserSession(browser_profile=profile)

		all_papers = []

		try:
			await browser_session.start()
			print('âœ… ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹\n')

			ieee_service = IEEESearchService()

			papers_per_query = max(5, self.max_papers // len(search_queries))

			for query_idx, query in enumerate(search_queries, 1):
				print(f'\nğŸ“ æ¤œç´¢ {query_idx}/{len(search_queries)}: "{query}"')

				try:
					# IEEEæ¤œç´¢ã‚’å®Ÿè¡Œ
					papers = await ieee_service.search(query=query, max_results=papers_per_query, browser_session=browser_session)

					print(f'  âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹')

					# æ¤œç´¢ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’è¿½åŠ 
					for paper in papers:
						paper['search_query'] = query

					all_papers.extend(papers)

				except Exception as e:
					logger.error(f'Error searching for "{query}": {e}')
					print(f'  âš ï¸  ã‚¨ãƒ©ãƒ¼: {e}')
					continue

				# ä¸Šé™ã«é”ã—ãŸã‚‰çµ‚äº†
				if len(all_papers) >= self.max_papers:
					print(f'\nğŸ¯ ç›®æ¨™è«–æ–‡æ•° {self.max_papers} ã«åˆ°é”')
					break

				# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å¾…æ©Ÿ
				await asyncio.sleep(2)

		finally:
			await browser_session.kill()
			print('\nâœ… ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†')

		# é‡è¤‡é™¤å»
		unique_papers = self._deduplicate_papers(all_papers)
		unique_papers = unique_papers[: self.max_papers]

		print(f'\nâœ… åˆè¨ˆ {len(unique_papers)} ä»¶ã®è«–æ–‡ã‚’åé›†')
		print(f'   ï¼ˆé‡è¤‡é™¤å»å‰: {len(all_papers)} ä»¶ï¼‰')

		# ä¿å­˜
		output_path = self.data_dir / f'collected_papers_{self.session_id}.json'
		with open(output_path, 'w', encoding='utf-8') as f:
			json.dump({'papers': unique_papers, 'total_count': len(unique_papers)}, f, indent=2, ensure_ascii=False)

		print(f'\nğŸ’¾ è«–æ–‡æƒ…å ±ã‚’ä¿å­˜: {output_path}')

		return unique_papers

	async def _step4_generate_reports(self, papers: list[dict[str, Any]], research_info: dict[str, Any]) -> list[dict[str, str]]:
		"""ã‚¹ãƒ†ãƒƒãƒ—4: å„è«–æ–‡ã®è½åˆé™½ä¸€å¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
		print('\n' + 'ğŸ“ ' * 30)
		print('ã‚¹ãƒ†ãƒƒãƒ— 4/5: å„è«–æ–‡ã®è©³ç´°åˆ†æï¼ˆè½åˆé™½ä¸€å¼ï¼‰')
		print('ğŸ“ ' * 30 + '\n')

		generator = OchiaiReportGenerator(llm=self.llm)
		reports = []

		# ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
		session_reports_dir = self.reports_dir / f'session_{self.session_id}'
		session_reports_dir.mkdir(parents=True, exist_ok=True)

		for idx, paper in enumerate(papers, 1):
			print(f'\n[{idx}/{len(papers)}] åˆ†æä¸­: {paper.get("title", "Unknown")[:80]}...')

			try:
				# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
				report = await generator.generate_paper_report(paper, research_info, pdf_content=None)

				# ä¿å­˜
				safe_title = ''.join(c if c.isalnum() or c in ' -_' else '_' for c in paper.get('title', 'paper'))
				safe_title = safe_title[:50]
				filename = f'{idx:03d}_{safe_title}.md'

				report_path = session_reports_dir / filename
				with open(report_path, 'w', encoding='utf-8') as f:
					f.write(report)

				reports.append({'paper_info': paper, 'report': report, 'file_path': str(report_path)})

				print(f'  âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path.name}')

			except Exception as e:
				logger.error(f'Error generating report for paper {idx}: {e}')
				print(f'  âš ï¸  ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}')
				continue

			# APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
			if idx < len(papers):
				await asyncio.sleep(1)

		print(f'\nâœ… {len(reports)}/{len(papers)} ä»¶ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ')

		return reports

	async def _step5_generate_summary(
		self, reports: list[dict[str, str]], research_info: dict[str, Any], search_strategy: dict[str, Any]
	) -> None:
		"""ã‚¹ãƒ†ãƒƒãƒ—5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
		print('\n' + 'ğŸ“Š ' * 30)
		print('ã‚¹ãƒ†ãƒƒãƒ— 5/5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ')
		print('ğŸ“Š ' * 30 + '\n')

		generator = OchiaiReportGenerator(llm=self.llm)

		# ã™ã¹ã¦ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
		all_report_texts = [r['report'] for r in reports]

		# çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
		summary_report = await generator.generate_summary_report(all_report_texts, research_info, search_strategy)

		# ä¿å­˜
		summary_path = self.reports_dir / f'summary_report_{self.session_id}.md'
		with open(summary_path, 'w', encoding='utf-8') as f:
			f.write(summary_report)

		print(f'\nâœ… çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ: {summary_path}')

		# è¿½åŠ ï¼šè«–æ–‡ãƒªã‚¹ãƒˆä¸€è¦§ã‚‚JSONå½¢å¼ã§ä¿å­˜
		papers_list = [r['paper_info'] for r in reports]
		papers_list_path = self.reports_dir / f'papers_list_{self.session_id}.json'
		with open(papers_list_path, 'w', encoding='utf-8') as f:
			json.dump({'papers': papers_list, 'total': len(papers_list)}, f, indent=2, ensure_ascii=False)

		print(f'âœ… è«–æ–‡ãƒªã‚¹ãƒˆã‚’ä¿å­˜: {papers_list_path}')

	def _deduplicate_papers(self, papers: list[dict[str, Any]]) -> list[dict[str, Any]]:
		"""ã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã§é‡è¤‡é™¤å»"""
		seen_titles = set()
		unique_papers = []

		for paper in papers:
			title = paper.get('title', '').lower().strip()
			if title and title not in seen_titles:
				seen_titles.add(title)
				unique_papers.append(paper)

		return unique_papers

	def _print_completion_summary(self) -> None:
		"""å®Œäº†ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
		print('\n\n' + '=' * 100)
		print('ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼')
		print('=' * 100)
		print('\nç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:')
		print(f'  ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.data_dir}')
		print(f'  ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.reports_dir}')
		print('\nä¸»è¦ãªæˆæœç‰©:')
		print(f'  ğŸ“ ç ”ç©¶æƒ…å ±: {self.data_dir}/research_info_{self.session_id}.json')
		print(f'  ğŸ“Š æ¤œç´¢æˆ¦ç•¥: {self.data_dir}/search_strategy_{self.session_id}.json')
		print(f'  ğŸ“š åé›†è«–æ–‡: {self.data_dir}/collected_papers_{self.session_id}.json')
		print(f'  ğŸ“„ å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆ: {self.reports_dir}/session_{self.session_id}/')
		print(f'  ğŸ“– çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ: {self.reports_dir}/summary_report_{self.session_id}.md')
		print('\n' + '=' * 100 + '\n')


async def main():
	"""ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
	import argparse
	import os

	# ãƒ–ãƒ©ã‚¦ã‚¶ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å»¶é•·ï¼ˆç’°å¢ƒå¤‰æ•°ã§è¨­å®šï¼‰
	# ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•é–¢é€£ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
	os.environ.setdefault('TIMEOUT_BrowserStartEvent', '180')  # 180ç§’ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•å…¨ä½“ï¼‰
	os.environ.setdefault('TIMEOUT_BrowserLaunchEvent', '180')  # 180ç§’ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•ï¼‰
	os.environ.setdefault('TIMEOUT_CDP_URL_WAIT', '180')  # 180ç§’ï¼ˆCDP URLå¾…æ©Ÿï¼‰
	os.environ.setdefault('TIMEOUT_BrowserConnectedEvent', '120')  # 120ç§’ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶æ¥ç¶šï¼‰
	os.environ.setdefault('TIMEOUT_TabCreatedEvent', '60')  # 60ç§’ï¼ˆã‚¿ãƒ–ä½œæˆï¼‰

	# ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œé–¢é€£
	os.environ.setdefault('TIMEOUT_NavigateToUrlEvent', '90')  # 90ç§’ï¼ˆãƒšãƒ¼ã‚¸é·ç§»ï¼‰
	os.environ.setdefault('TIMEOUT_NavigationStartedEvent', '60')  # 60ç§’
	os.environ.setdefault('TIMEOUT_NavigationCompleteEvent', '90')  # 90ç§’
	os.environ.setdefault('TIMEOUT_BrowserStateRequestEvent', '120')  # 120ç§’
	os.environ.setdefault('TIMEOUT_ClickElementEvent', '30')  # 30ç§’
	os.environ.setdefault('TIMEOUT_UploadFileEvent', '60')  # 60ç§’

	# ãã®ä»–ã®ã‚¤ãƒ™ãƒ³ãƒˆ
	os.environ.setdefault('TIMEOUT_BrowserKillEvent', '30')  # 30ç§’
	os.environ.setdefault('TIMEOUT_BrowserStoppedEvent', '30')  # 30ç§’
	os.environ.setdefault('TIMEOUT_BrowserErrorEvent', '30')  # 30ç§’
	os.environ.setdefault('TIMEOUT_StorageStateSavedEvent', '60')  # 60ç§’
	os.environ.setdefault('TIMEOUT_StorageStateLoadedEvent', '60')  # 60ç§’
	os.environ.setdefault('TIMEOUT_FileDownloadedEvent', '120')  # 120ç§’ï¼ˆè«–æ–‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰

	parser = argparse.ArgumentParser(description='å®Œå…¨è‡ªå‹•åŒ–ç ”ç©¶æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ')
	parser.add_argument('--headless', action='store_true', help='ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ')
	parser.add_argument('--max-papers', type=int, default=20, help='åé›†ã™ã‚‹æœ€å¤§è«–æ–‡æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰')
	parser.add_argument(
		'--provider', type=str, default=None, help='LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆopenai, claude, deepseek, google, groqï¼‰'
	)
	parser.add_argument('--model', type=str, default=None, help='ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯çœç•¥å¯ï¼‰')
	parser.add_argument('--non-interactive', action='store_true', help='éå¯¾è©±å‹ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ¢ç”¨ã®äº‹å‰å®šç¾©ã•ã‚ŒãŸç ”ç©¶æƒ…å ±ã‚’ä½¿ç”¨ï¼‰')
	parser.add_argument('--research-topic', type=str, default=None, help='ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ--non-interactiveã¨ä½µç”¨ï¼‰')

	args = parser.parse_args()

	# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
	print_provider_info()

	# LLMåˆæœŸåŒ–ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ï¼‰
	llm = get_llm(provider=args.provider, model=args.model, temperature=0.4)

	# ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
	assistant = AutomatedResearchAssistant(
		llm=llm, headless=args.headless, max_papers=args.max_papers, non_interactive=args.non_interactive, research_topic=args.research_topic
	)

	# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
	await assistant.run_full_pipeline()


if __name__ == '__main__':
	asyncio.run(main())
